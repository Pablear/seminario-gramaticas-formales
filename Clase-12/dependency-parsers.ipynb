{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Parsers de dependencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requerimientos\n",
    "- Python 3\n",
    "- Spacy\n",
    "- NLTK\n",
    "- MaltParser\n",
    "- Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk import Tree\n",
    "from spacy import displacy \n",
    "from nltk.parse import malt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No poseen distinción entre símbolos no terminales y terminales. Las estructuras representan relaciones de dependencia entre terminales.\n",
    "Ejemplos de parsers de dependencias:\n",
    "* [Projective Dependency Parser de NLTK](https://www.nltk.org/_modules/nltk/parse/projectivedependencyparser.html)\n",
    "* [Maltparser](http://www.maltparser.org/)\n",
    "* SyntaxNet (Estaba alojado en https://opensource.google.com/projects/syntaxnet, como parte de los recursos de la librería para Inteligencia Artificial TensorFlow de Google, pero en este momento no está disponible y [se rumorea](https://github.com/tensorflow/models/issues/8411) que se lo va a mover al github de [google-research](https://github.com/google-research/google-research))\n",
    "* [Dependency parser de Spacy](https://spacy.io/usage/linguistic-features#dependency-parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projective Dependency Parser NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_parser(sentence, grammar):         # define una función llamada dep_parser con dos argumentos\n",
    "    sentence = sentence.lower()            # convierte a minúscula la oración\n",
    "    if sentence.endswith('.'):             # si la oración termina con un punto\n",
    "        sent = re.sub('\\.','',sentence)    # se lo quita\n",
    "    else:                                  # si no\n",
    "        sent = sentence                    # la toma como está\n",
    "    sent = sent.split()                    # divide la oración en palabras\n",
    "    dep_gram = nltk.data.load(grammar, cache=False) # carga la gramática a nltk\n",
    "    dep_gram = nltk.DependencyGrammar.fromstring(dep_gram) # parsea la gramática como gramática de dependencias\n",
    "    pdp = nltk.ProjectiveDependencyParser(dep_gram) # aarga la gramática en el parser\n",
    "    for tree in pdp.parse(sent):           # para cada árbol posible en mi gramática para esa oración\n",
    "        print(tree)                        # lo imprime\n",
    "        return(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para correr el Proyective Dependency Parser\n",
    "\n",
    "oracion1 = 'Pablo explotó el globo'    # Define la oración a analizar\n",
    "grammar = 'gramaticas/DG1.txt'        # establece cuál va a ser mi gramática\n",
    "dep_parser(oracion1, grammar)          # Para correr la función"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Projective Dependency Parser NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npdep_parser(sentence, grammar):                # define una función llamada dep_parser con dos argumentos\n",
    "    sentence = sentence.lower()                     # convierte a minúscula la oración\n",
    "    if sentence.endswith('.'):                      # si la oración termina con un punto\n",
    "        sent = re.sub('\\.',' ',sentence)            # se lo quita\n",
    "    else:                                           # si no\n",
    "        sent = sentence                             # la toma como está\n",
    "    sent = sent.split()                             # divide la oración en palabras\n",
    "    dep_gram = nltk.data.load(grammar, cache=False) # carga la gramática a nltk\n",
    "    dep_gram = nltk.DependencyGrammar.fromstring(dep_gram) # parsea la gramática como gramática de dependencias\n",
    "    pdp = nltk.NonprojectiveDependencyParser(dep_gram) # carga la gramática en el parser\n",
    "    for tree in pdp.parse(sent):\n",
    "        print(tree.tree().draw())\n",
    "    return tree.tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para correr el Nonproyective Dependency Parser\n",
    "\n",
    "#oracion2 = 'quién fuma el cigarrillo'  # Define la oración a analizar\n",
    "#oracion2 = 'quién dijo fede que fuma'  # Define la oración a analizar\n",
    "oracion2 = 'qué dijo fede que fuma'  # Define la oración a analizar\n",
    "# Habría que arreglar la función npdep_parser para que pueda tomar estas dos últimas oraciones\n",
    "grammar2 = 'gramaticas/DG2.txt'       # establece cuál va a ser mi gramática\n",
    "npdep_parser(oracion2, grammar2)        # Para correr la función"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PystanfordDependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stanza.download('es') # Baja el modelo para el españo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline('es') # Inicializa el modelo de español (con su pipeline de anotación)\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Pablo Neruda escribe poemas en Capri\") # Anota una oración\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.sentences[0].dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"el poeta chileno escribe poemas.\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_entidades(stanza_oracion):\n",
    "    entidades = [] # Creo una lista vacía donde voy a guardar todas las entidades que encuentre en la oración\n",
    "    for dependencia in stanza_oracion.dependencies: # Recorro las dependencias\n",
    "        regidor, relacion, dependiente = dependencia # Las dependencias son tuplas de tres elementos que puedo separar\n",
    "                                                     # en variables\n",
    "        if regidor.deprel == \"nsubj\": # Asumimos que queremos encontrar los sujetos, pero podríamos comparar por PoS\n",
    "                                      # ¿Cómo?\n",
    "            entidad = [regidor]       # Creo una lista cuyo miembro inicial es el nucleo de la construccion\n",
    "            for palabra in stanza_oracion.words: # Vuelvo a recorrer las palabras de la oración para encontrar\n",
    "                                                 # todos los dependientes del sujeto, sean anteriores o posteriores\n",
    "                if palabra.head == int(regidor.id): # Si el nucleo/regidor/head de una palabra coincide con el id\n",
    "                    entidad.append(palabra)         # del núcleo de mi construcción, lo sumo a la lista de la entidad\n",
    "            entidad = sorted(entidad, key=lambda x: x.id) # Ordeno la lista resultante por su número de id para \n",
    "                                                          # mantener la linealidad del texto.\n",
    "            if entidad not in entidades:            # Chequeo que la entidad ya no exista en mi lista porque\n",
    "                entidades.append(entidad)           # el regidor puede aparecer en más de una dependencia.\n",
    "            \n",
    "    return [\" \".join([palabra.text for palabra in entidad]) for entidad in entidades] # Me quedo únicamente con el \n",
    "                                                       # texto de cada objeto Word y lo devuelvo unido por entidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraer_entidades(doc.sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.sentences[0].words[0].head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FreeLing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FreeLing posee su propio server gratuito que disponibiliza el pipeline de la librería. También se puede descargar\n",
    "y correr como un programa local e incluso cuenta con una implementación en Python. Hoy vamos a probar FreeLing \n",
    "en su [demo visual](https://nlp.lsi.upc.edu/freeling/demo/demo.php).\n",
    "\n",
    "Si hay dudas con el uso de tags de PoS, se pueden revisar rápudamente en esta página: [Descripción del Tagset](https://www.sketchengine.eu/spanish-freeling-part-of-speech-tagset/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sugerencias para probar:\n",
    "# A las 4 de la tarde tomo leche y como tostadas.\n",
    "# No tengo más sueño.\n",
    "# ¿A quién dijiste que viste el sábado?\n",
    "# Dije que vi a mi tía.\n",
    "# Pablo Neruda escribe poemas en Capri.\n",
    "# pablo neruda escribe poemas en capri."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Malt Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación:\n",
    "\n",
    "- Descargamos Malt Parser en su versión 1.9.2 desde http://maltparser.org/dist/maltparser-1.9.2.tar.gz y lo descomprimimos con el comando `tar`\n",
    "- El proceso anterior nos tiene que haber creado una carpeta llamada `maltparser-1.9.2`\n",
    "- Bajamos modelo entrenado engmalt.poly-1.7 de http://www.maltparser.org/mco/english_parser/engmalt.poly-1.7.mco a la carpeta `maltparser-1.9.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descarga maltparser desde la url indicada en una carpeta llamada maltparser-1.9.2 (la crea)\n",
    "! wget -qO- http://maltparser.org/dist/maltparser-1.9.2.tar.gz | tar -xvz > /dev/null\n",
    "\n",
    "# descarga el modelo engmalt.poly-1.7 en la carpeta maltparser-1.9.2\n",
    "! wget -P maltparser-1.9.2 http://www.maltparser.org/mco/english_parser/engmalt.poly-1.7.mco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- obtenemos el path absoluto a donde está ubicada esta notebook (`os.path.abspath('.')`)\n",
    "- unimos esa ubicación con el nombre de la carpeta `maltparser-1.9.2` y hacemos lo mismo con ese resultado y el archivo del modelo (`engmalt.poly-1.7.mco`)\n",
    "- alojamos ambos paths como variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "here = os.path.abspath('.')\n",
    "print('here es: ', here)\n",
    "maltparser_folder = 'maltparser-1.9.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getenv('MALT_PARSER'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MALT_PARSER'] = os.path.join(here, maltparser_folder, '')\n",
    "os.environ['MALT_MODEL'] = os.path.join(here, maltparser_folder, 'engmalt.poly-1.7.mco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getenv('MALT_PARSER'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo $MALT_PARSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maltParser = nltk.parse.malt.MaltParser(os.getenv('MALT_PARSER'), os.getenv('MALT_MODEL'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracion_malt = 'John saw Mary with her new dress'\n",
    "stemma = maltParser.parse_one(oracion_malt.split()).tree()\n",
    "print(stemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy - Dependency parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota para quien no tenga la MV:**\n",
    "\n",
    "Antes de correr hay que instalar spacy. Con pip3, eso se puede hacer con el comando \n",
    "\n",
    "`pip3 install spacy`\n",
    "\n",
    "Hay que instalar también es_core_news_sm, un modelo entrenado mediante un corpus del español, con el comando\n",
    "\n",
    "`python3 -m spacy download es_core_news_sm`\n",
    "\n",
    "Alternativamente puede probarse de instalar es_core_news_md.\n",
    "\n",
    "`python3 -m spacy download es_core_news_md`\n",
    "\n",
    "En ese caso, para correrlo hay que cambiar en el código de abajo `es_core_news_sm` por `es_core_news_md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el modelo\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Procesa la oración\n",
    "sent = \"¿quiénes dijo Juan que estornudaron?\"\n",
    "doc = nlp(sent)\n",
    "\n",
    "#options = {\"compact\": True, \"bg\": \"#09a3d5\",\"color\": \"white\", \"font\": \"Source Sans Pro\"}\n",
    "# Visualización\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gramaticadependencias(sentence):                # Define la función\n",
    "    nlp = spacy.load('es_core_news_sm')             # Carga el modelo entrenado\n",
    "    doc = nlp(sentence)                             # Procesa la oración con el modelo\n",
    "    displacy.render(doc, style='dep', jupyter=True) # Visualiza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracion_spacy = input('Escribí una oración\\n')\n",
    "gramaticadependencias(oracion_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')\n",
    "doc = nlp(oracion_spacy)\n",
    "root = [token for token in doc if token.head == token][0]\n",
    "print('| {0:10}| {1:5}| {2:7}| {3:7}| {4:30} |'.format('TEXTO','DEP','N_IZQ','N_DER','ANCESTROS'))\n",
    "print('|'+'='*69+'|')\n",
    "for descendant in root.subtree:\n",
    "    print('| {0:10}| {1:5}| {2:7}| {3:7}| {4:30} |'.format(\n",
    "        descendant.text,\n",
    "        descendant.dep_,\n",
    "        descendant.n_lefts,\n",
    "        descendant.n_rights,\n",
    "        str([ancestor.text for ancestor in descendant.ancestors])\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
